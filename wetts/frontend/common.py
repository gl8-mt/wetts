# BERT_PRETRAIN_MODEL = 'bert-base-chinese'

## === 基于全词掩码（Whole Word Masking）技术的中文预训练模型BERT-wwm ===
#
# https://github.com/ymcui/Chinese-BERT-wwm
#
# BERT_PRETRAIN_MODEL = 'hfl/chinese-roberta-wwm-ext-large'
# BERT_PRETRAIN_MODEL = 'hfl/chinese-roberta-wwm-ext'

## === MT ===
#
# BERT_PRETRAIN_MODEL = '/nfs1/nlp/models/MTBert_v0.0.1'
# BERT_PRETRAIN_MODEL = '/nfs1/nlp/models/MTBert_v0.0.3'
# BERT_PRETRAIN_MODEL = '/nfs1/nlp/models/deberta'
# BERT_PRETRAIN_MODEL = '/nfs1/nlp/models/deberta-small'
BERT_PRETRAIN_MODEL = '/nfs1/nlp/models/deberta-xsmall'
